# -*- coding: utf-8 -*-
"""lvadsusr139_cluster_anbu.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aRFXJQhh_9XVE0CFvC538md-TUnT5Zta

Importing Libraries
"""

import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt

import warnings
warnings.filterwarnings("ignore")

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler,LabelEncoder
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

"""Reading Data"""

data= pd.read_csv('https://raw.githubusercontent.com/Deepsphere-AI/LVA-Batch5-Assessment/main/customer_segmentation.csv')

"""Describe"""

data.describe()

"""Info"""

data.info()

"""Missing Values"""

data.isna().sum()# missing values in Income tab

# Handle numerical columns
numerical_columns = data.select_dtypes(include=['int64', 'float64'])
for column in numerical_columns.columns:
    data[column] = data[column].fillna(data[column].mean())

# Handle categorical columns
categorical_columns = data.select_dtypes(include=['object'])
for column in categorical_columns.columns:
    data[column] = data[column].fillna(data[column].mode())

data.isnull().sum()

"""Duplicates"""

data.duplicated().sum()#no duplicate values in data set

len = LabelEncoder()
for column in data.select_dtypes(include = 'object'):
  data[column] = len.fit_transform(data[column])
data.head()

scaler = MinMaxScaler()
for column in data.select_dtypes(include=['float64','int64']):
  data[column] = scaler.fit_transform(data[[column]])

print(data.head())

data.columns

"""Correlation Matrix"""

numerical_columns= data.select_dtypes(include=['int','float'])

correl = numerical_columns.corr()
print(correl)

correl = numerical_columns.corr()

plt.figure(figsize=(12, 8))
sns.heatmap(correl, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Heatmap")
plt.show()
print()

"""Outliers box plot and removal"""

sns.boxplot(data=data)
plt.title("Boxplot of Variables")
plt.xticks(rotation=45)
plt.show()
print()

for col in data.select_dtypes(include=[np.number]).columns:
    ninety_fifth_percentile = data[col].quantile(0.95)
    data[col] = np.where(data[col] > ninety_fifth_percentile, ninety_fifth_percentile, data[col])

sns.boxplot(data=data)
plt.title("Boxplot of Variables")
plt.xticks(rotation=45)
plt.show()
print()

"""Scaling(normalization)"""

sns.boxplot(data=data)
plt.title("Boxplot of Variables after Handling Outliers")
plt.xticks(rotation=45)
plt.show()
print()

data.select_dtypes(['object']).columns

from sklearn.preprocessing import LabelEncoder, StandardScaler
import pandas as pd

# Initialize LabelEncoder
l = LabelEncoder()

# Encode categorical columns
data_encoded = pd.DataFrame()  # Initialize a new DataFrame to store encoded data
for column in data.select_dtypes(include='object').columns:
    data_encoded[column] = l.fit_transform(data[column])

data_encoded.head()

# Initialize StandardScaler
scal = StandardScaler()

# Scale numerical columns
for column in data.select_dtypes(include=['int','float']).columns:
    data[column] = scal.fit_transform(data[[column]])

# Convert the scaled array back into a DataFrame
scaled_data = pd.DataFrame(data=data_encoded, columns=data.columns)

scaled_data.dropna()

sse = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, random_state=42)
    kmeans.fit(scaled_data)
    sse.append(kmeans.inertia_)

plt.plot(range(1, 11), sse, marker='o', linestyle='--')
plt.title('Elbow plot')
plt.xlabel('Number of Clusters')
plt.ylabel('Sum of squared error (SSE)')
plt.xticks(range(1, 11))
plt.grid(True)
plt.show()
print()

km = KMeans(n_clusters=2)
y_predicted = km.fit_predict(data[['NumDealsPurchases','Marital_Status']])
y_predicted = km.fit_predict(data[['NumDealsPurchases','Marital_Status']])
y_predicted
data['cluster']=y_predicted
data.head(25)
print(km.cluster_centers_)

df1 = data[data.cluster==0]
df2 = data[data.cluster==1]
df3 = data[data.cluster==2]
df4 = data[data.cluster==3]
plt.scatter(df1.status_type,df1['NumDealsPurchases'],color='green')
plt.scatter(df2.status_type,df2['NumDealsPurchases'],color='red')
plt.scatter(df3.status_type,df3['NumDealsPurchases'],color='black')
plt.scatter(df4.status_type,df4['NumDealsPurchases'],color='black')
plt.scatter(km.cluster_centers_[:,0],km.cluster_centers_[:,1],color='purple',marker='*',label='centroid')
plt.xlabel('Marital_Status')
plt.ylabel('NumDealsPurchases')
plt.legend()

from sklearn.metrics import silhouette_score

sil = silhouette_score(data,y_pred)
print(sil)